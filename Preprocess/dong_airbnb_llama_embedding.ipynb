{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hj/anaconda3/envs/cs612/lib/python3.9/site-packages/transformers/models/auto/configuration_auto.py:1002: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hj/anaconda3/envs/cs612/lib/python3.9/site-packages/transformers/models/auto/tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/home/hj/anaconda3/envs/cs612/lib/python3.9/site-packages/transformers/models/auto/auto_factory.py:471: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280d099b96574ed88e8eb6be79306c6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Set your Hugging Face access token\n",
    "os.environ['HF_TOKEN'] = ''\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "\n",
    "# Load the model and tokenizer with memory efficient settings\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "\n",
    "# Create config with memory optimizations\n",
    "config = AutoConfig.from_pretrained(model_id, use_auth_token=os.environ['HF_TOKEN'])\n",
    "config.use_cache = False\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_auth_token=os.environ['HF_TOKEN'])\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Initialize model with memory optimizations\n",
    "model = AutoModel.from_pretrained(\n",
    "    model_id,\n",
    "    config=config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    use_auth_token=os.environ['HF_TOKEN'],\n",
    "    low_cpu_mem_usage=True\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and process the input file\n",
    "with open('../../prompts/new_prompts/raw_prompts_new.txt', 'r') as file:\n",
    "    content = file.read()\n",
    "\n",
    "prompts = content.split('--------------------------------------------------')\n",
    "prompts = [prompt.strip() for prompt in prompts if prompt.strip()]\n",
    "\n",
    "# Configure numpy to prevent scientific notation and set high precision\n",
    "np.set_printoptions(suppress=True, precision=8, threshold=np.inf, linewidth=np.inf)\n",
    "\n",
    "# Load dong names dynamically from dataset\n",
    "def get_dong_names():\n",
    "    df = pd.read_csv('../Data/Raw_data/AirBnB_data.csv')\n",
    "    return list(df['Dong_name'].unique())\n",
    "\n",
    "dong_names = get_dong_names()\n",
    "\n",
    "# Define date range\n",
    "date_range = pd.date_range(start='2017-01-01', end='2022-07-01', freq='MS')\n",
    "date_range = date_range.strftime('%Y-%m-%d')\n",
    "\n",
    "# Create full index for all possible combinations of dates and dongs\n",
    "full_index = pd.MultiIndex.from_product([date_range, dong_names], names=['Reporting Month', 'Dong_name'])\n",
    "full_df = pd.DataFrame(index=full_index).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing prompts:   0%|          | 0/37096 [00:00<?, ?it/s]/tmp/ipykernel_1012012/430908506.py:17: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.no_grad(), torch.cuda.amp.autocast():  # Use automatic mixed precision\n",
      "Processing prompts: 100%|██████████| 37096/37096 [1:01:17<00:00, 10.09it/s]\n"
     ]
    }
   ],
   "source": [
    "# Open the output CSV file\n",
    "output_file = 'raw_embeddings_new.csv'\n",
    "with open(output_file, 'w', newline='') as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # Write the header row\n",
    "    max_embedding_length = 3072  # Replace with the actual maximum length of your embeddings if known\n",
    "    header = [\"Dong_name\", \"Reporting Month\"] + [f\"LLM Embeddings_{i}\" for i in range(max_embedding_length)]\n",
    "    csv_writer.writerow(header)\n",
    "\n",
    "    # Collect embeddings to merge later\n",
    "    embedding_rows = []\n",
    "\n",
    "    # Process each prompt\n",
    "    for i, prompt in enumerate(tqdm(prompts, desc=\"Processing prompts\")):\n",
    "        try:\n",
    "            # Move input tensors to the same device as the model\n",
    "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            # Process one embedding at a time\n",
    "            embedding = outputs.last_hidden_state.mean(dim=1).to(torch.float32).cpu().numpy()\n",
    "\n",
    "            # Pad or trim embedding to match the header length\n",
    "            embedding = embedding.flatten()\n",
    "            if len(embedding) < max_embedding_length:\n",
    "                embedding = np.pad(embedding, (0, max_embedding_length - len(embedding)), constant_values=0)\n",
    "            else:\n",
    "                embedding = embedding[:max_embedding_length]\n",
    "\n",
    "            # Prepare the row data (replace placeholder values with actual prompt details if available)\n",
    "            row = [\"Dong_name_placeholder\", \"Reporting Month_placeholder\"] + embedding.tolist()\n",
    "            embedding_rows.append(row)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing prompt {i}: {str(e)}\")\n",
    "\n",
    "        # Clear CUDA cache periodically\n",
    "        if i % 100 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "# Convert embedding rows to a DataFrame\n",
    "dong_embedding = pd.DataFrame(embedding_rows, columns=header)\n",
    "\n",
    "# Merge with the full DataFrame to ensure all dongs and dates are included\n",
    "merged_df = pd.merge(full_df, dong_embedding, on=['Reporting Month', 'Dong_name'], how='left')\n",
    "merged_df.fillna(0, inplace=True)\n",
    "\n",
    "# Write the merged DataFrame to the CSV\n",
    "merged_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cs612)",
   "language": "python",
   "name": "cs612"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
